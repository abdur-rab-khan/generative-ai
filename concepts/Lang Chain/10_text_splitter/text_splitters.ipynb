{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7962ce3",
   "metadata": {},
   "source": [
    "# Approaches to Split Documents\n",
    "\n",
    "1. [Length Based](#length-based)\n",
    "2. [Text-structure Based](#text-structure-based)\n",
    "3. [Document-structure Based](#)\n",
    "4. [Semantic meaning](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8444e56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Length Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9ecc6b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the document from pdf\n",
    "from os import path\n",
    "\n",
    "pdf_path = \"../sample_files/progit.pdf\"\n",
    "# pdf_path = \"../sample_files/welcome.pdf\"\n",
    "\n",
    "if not path.exists(pdf_path):\n",
    "    raise Exception(\"Invalid path, File does not exits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "db5f29df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "\n",
    "docs = \"\"\n",
    "metadatas = []\n",
    "list_docs = []\n",
    "\n",
    "\n",
    "async for doc in loader.alazy_load():\n",
    "    current_page_number = doc.metadata[\"page_label\"]\n",
    "\n",
    "    if current_page_number.isdigit():\n",
    "        current_page_number = int(current_page_number)\n",
    "\n",
    "        if current_page_number < 8:\n",
    "            continue\n",
    "\n",
    "        if current_page_number >= 492:\n",
    "            break\n",
    "\n",
    "        docs += doc.page_content\n",
    "\n",
    "        list_docs.append(doc.page_content)\n",
    "        metadatas.append(doc.metadata)\n",
    "    else:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b15200",
   "metadata": {},
   "source": [
    "### Load the document from `PDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ffb67b",
   "metadata": {},
   "source": [
    "### 1. Split text by token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c6abe0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token-based: 442 chunks\n",
      "Introduction\n",
      "You’re about to spend several hours of your life reading about Git. Let’s take a minute to explain\n",
      "what we have in store for you. Here is a quick summary of the ten chapters and three appendices of\n",
      "this book.\n",
      "In Chapter 1 , we’re going to cover Version Control Systems (VCSs) and Git basics — no technical\n",
      "stuff, just what Git is, why it came about in a land full of VCSs, what sets it apart, and why so many\n",
      "people are using it. Then, we’ll explain how to download Git and set it up for\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "\n",
    "# Define the token splitter\n",
    "token_splitter = TokenTextSplitter.from_tiktoken_encoder(\n",
    "    encoding_name=\"cl100k_base\",\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=25\n",
    ")\n",
    "\n",
    "token_chunks = token_splitter.split_text(docs)\n",
    "\n",
    "print(f\"Token-based: {len(token_chunks)} chunks\")\n",
    "print(token_chunks[0][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72958dd",
   "metadata": {},
   "source": [
    "### Create **`document`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "724fbe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Chunk size: 666\n",
      "Content is: Introduction\n",
      "You’re about to spend several hours of your life reading about Git. Let’s take a minute to explain\n",
      "what we have in store for you. Here is a quick summary of the ten chapters and three app\n"
     ]
    }
   ],
   "source": [
    "doc_chunks = token_splitter.create_documents(list_docs, metadatas=metadatas)\n",
    "print(f\"Total Chunk size: {len(doc_chunks)}\")\n",
    "print(f\"Content is: {doc_chunks[0].page_content[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab496d3",
   "metadata": {},
   "source": [
    "- In **`.from_tiktoken_encorder()`**\n",
    "  - It takes **`encoding_name`** eg **\"cl100k_base\"** or **`model_name`** eg **\"gpt-4\"**\n",
    "- **`chunk_size`** define the size of token.\n",
    "- **`chunk_overlap`** defines the how many characters or tokens from the end of one chunk repeat at beginning of the next chunk."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7b9626",
   "metadata": {},
   "source": [
    "- We can also use **`TokenTextSplitter`** splitter which is directly work with **`tiktoken`**. Also ensure each chunk should smaller than `chunk_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "78424ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character-based: 20954 chunks\n",
      "String token: Preface by Ben Straub\n",
      "The first edition\n",
      "Document tokens: page_content='Preface by Ben Straub\n",
      "The first edition'\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import TokenTextSplitter\n",
    "\n",
    "splitter = TokenTextSplitter(\n",
    "    model_name=\"gpt-4\",\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "token_chunks = splitter.split_text(docs)\n",
    "chunk_docs = splitter.create_documents(token_chunks)\n",
    "print(f\"Character-based: {len(token_chunks)} chunks\")\n",
    "print(f\"String token: {token_chunks[0]}\")  # Show first 300 chars of the first\n",
    "print(f\"Document tokens: {chunk_docs[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a6d07c",
   "metadata": {},
   "source": [
    "### 2. Based by Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff61ee1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtext_splitter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CharacterTextSplitter\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTotal length of the docs is:- \u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mdocs\u001b[49m))\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Define the character splitter\u001b[39;00m\n\u001b[32m      5\u001b[39m char_splitter = CharacterTextSplitter(\n\u001b[32m      6\u001b[39m     separator=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     chunk_size=\u001b[32m1000\u001b[39m,      \u001b[38;5;66;03m# each chunk has 1000 characters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     10\u001b[39m     is_separator_regex=\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     11\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "print(\"Total length of the docs is:- \", len(docs))\n",
    "# Define the character splitter\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,      # each chunk has 1000 characters\n",
    "    chunk_overlap=200,     # each chunk overlaps 100 characters with the previous\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "char_chunks = char_splitter.split_text(docs)\n",
    "print(f\"Len chunks: {len(char_chunks)}\")\n",
    "print(f\"Char chunk: {char_chunks[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb784fe",
   "metadata": {},
   "source": [
    "- It you want to add metadata associated with each chunk use **`.create_documents`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120645cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'producer': 'Asciidoctor PDF 2.3.17, based on Prawn 2.4.0', 'creator': 'Scott Chacon, Ben Straub', 'creationdate': '2025-04-10T14:40:20+00:00', 'title': 'Pro Git', 'author': 'Scott Chacon, Ben Straub', 'moddate': '2025-04-10T14:40:04+00:00', 'source': '../sample_files/progit.pdf', 'total_pages': 501, 'page': 8, 'page_label': '3'}, page_content='Preface by Ben Straub\\nThe first edition of this book is what got me hooked on Git. This was my introduction to a style of\\nmaking software that felt more natural than anything I had seen before. I had been a developer for\\nseveral years by then, but this was the right turn that sent me down a much more interesting path\\nthan the one I was on.\\nNow, years later, I’m a contributor to a major Git implementation, I’ve worked for the largest Git\\nhosting company, and I’ve traveled the world teaching people about Git. When Scott asked if I’d be\\ninterested in working on the second edition, I didn’t even have to think.\\nIt’s been a great pleasure and privilege to work on this book. I hope it helps you as much as it did\\nme.\\n3Dedications\\nTo my wife, Becky, without whom this adventure never would have begun. — Ben\\nThis edition is dedicated to my girls. To my wife Jessica who has supported me for all of these years')]\n"
     ]
    }
   ],
   "source": [
    "doc_chunk = char_splitter.create_documents(\n",
    "    [char_chunks[0]], metadatas=[metadatas[0]])\n",
    "\n",
    "print(doc_chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc941009",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Text-structure Based\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "ce772cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define text splitter\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False\n",
    ")\n",
    "\n",
    "texts = splitter.create_documents([docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "067bd90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "vector_store = InMemoryVectorStore.from_documents(\n",
    "    texts, OpenAIEmbeddings(model=\"text-embedding-3-small\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "20377f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(id='e671f66b-f921-42f3-b545-f04ef662271d', metadata={}, page_content=\"all and commit. Staging the files computes a checksum for each one (the SHA-1 hash we mentioned\\nin What is Git?), stores that version of the file in the Git repository (Git refers to them as blobs), and\\nadds that checksum to the staging area:\\n$ git add README test.rb LICENSE\\n$ git commit -m 'Initial commit'\\nWhen you create the commit by running git commit, Git checksums each subdirectory (in this case,\"), Document(id='3360d11c-6d6d-4d3d-b63c-8d04e43b7de2', metadata={}, page_content=')\\ncommit = repo.lookup(commit_id) ⑧\\n① Create a new blob, which contains the contents of a new file.\\n② Populate the index with the head commit’s tree, and add the new file at the path newfile.txt.\\n③ This creates a new tree in the ODB, and uses it for the new commit.\\n④ We use the same signature for both the author and committer fields.\\n⑤ The commit message.\\n⑥ When creating a commit, you have to specify the new commit’s parents. This uses the tip of\\nHEAD for the single parent.'), Document(id='a8b8e344-57de-4ef0-af70-9a1a7b989788', metadata={}, page_content='commit snapshot.\\n• Committed means that the data is safely stored in your local database.\\nThis leads us to the three main sections of a Git project: the working tree, the staging area, and the\\nGit directory.\\nFigure 6. Working tree, staging area, and Git directory\\nThe working tree is a single checkout of one version of the project. These files are pulled out of the\\ncompressed database in the Git directory and placed on disk for you to use or modify.'), Document(id='bbb98046-8ee7-416c-a64c-25df113f1264', metadata={}, page_content='index.read_tree(repo.head.target.tree)\\nindex.add(:path => \\'newfile.txt\\', :oid => blob_id) ②\\nsig = {\\n\\xa0   :email => \"bob@example.com\",\\n\\xa0   :name => \"Bob User\",\\n\\xa0   :time => Time.now,\\n}\\ncommit_id = Rugged::Commit.create(repo,\\n\\xa0   :tree => index.write_tree(repo), ③\\n\\xa0   :author => sig,\\n\\xa0   :committer => sig, ④\\n\\xa0   :message => \"Add newfile.txt\", ⑤\\n\\xa0   :parents => repo.empty? ? [] : [ repo.head.target ].compact, ⑥\\n\\xa0   :update_ref => \\'HEAD\\', ⑦\\n)\\ncommit = repo.lookup(commit_id) ⑧')]\n"
     ]
    }
   ],
   "source": [
    "docs = vector_store.similarity_search(\"commit\")\n",
    "\n",
    "print(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bccfbd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Semantic meaning Based"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
