{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ad3ac56",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5166a638",
   "metadata": {},
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "683efe66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f46c3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Create a prompt template for the chat model to give the response of all message in a funny way\n",
    "\n",
    "example = [\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of France?\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"The capital of France is Paris, but I prefer to call it 'The City of Lights and Croissants!'\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of Germany?\"\n",
    "    ),\n",
    "    AIMessage(content=\"The capital of Germany is Berlin, but I like to think of it as 'The Land of Sausages and Beer!'\"\n",
    "              ),\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of Italy?\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"The capital of Italy is Rome, but I prefer to call it 'The Eternal City of Pizza and Pasta!'\"\n",
    "    ),\n",
    "    HumanMessage(\n",
    "        content=\"What is the capital of Spain?\"\n",
    "    ),\n",
    "    AIMessage(\n",
    "        content=\"The capital of Spain is Madrid, but I like to think of it as 'The Land of Flamenco and Tapas!'\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(\n",
    "            content=\"You are a helpful assistant that gives funny responses to all messages.\"\n",
    "        ),\n",
    "        *example,\n",
    "        MessagesPlaceholder(\"input\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3dcf1",
   "metadata": {},
   "source": [
    "## RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9953fe80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableSequence, RunnableLambda\n",
    "\n",
    "\n",
    "addition = RunnableLambda(lambda x: x + 10)\n",
    "multiplication = RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "chain = RunnableSequence(addition, multiplication)\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead82e0b",
   "metadata": {},
   "source": [
    "## RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9ceacfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'add': 20, 'mul': 20}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableParallel, RunnableLambda\n",
    "\n",
    "\n",
    "addition = RunnableLambda(lambda x: x + 10)\n",
    "multiplication = RunnableLambda(lambda x: x * 2)\n",
    "\n",
    "chain = RunnableParallel({\n",
    "    \"add\": addition,\n",
    "    \"mul\": multiplication\n",
    "})\n",
    "\n",
    "chain.invoke(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b1899",
   "metadata": {},
   "source": [
    "## Shorthand method or run these composition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a378a48",
   "metadata": {},
   "source": [
    "### The | Operator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fae9b6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of India is New Delhi, but I like to call it 'The City of Spicy Curries and Traffic Jams!'\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 28, 'prompt_tokens': 184, 'total_tokens': 212, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None}, id='run-c143f350-9233-4df2-871f-7f224ff741a8-0', usage_metadata={'input_tokens': 184, 'output_tokens': 28, 'total_tokens': 212, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We know that both prompt and llm are build using runnable interface\n",
    "\n",
    "chain = prompt | llm\n",
    "\n",
    "chain.invoke({\"input\": [HumanMessage(content=\"what is the capital of india\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f56ba",
   "metadata": {},
   "source": [
    "### The `.pipe()` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c6dad96b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"The capital of India is New Delhi! But I like to refer to it as 'The City of Curries and Chaos!'\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 184, 'total_tokens': 210, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_dbaca60df0', 'finish_reason': 'stop', 'logprobs': None}, id='run-a8a91dc7-5da1-499f-8289-cde957dfabdf-0', usage_metadata={'input_tokens': 184, 'output_tokens': 26, 'total_tokens': 210, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt.pipe(llm)\n",
    "\n",
    "chain.invoke({\"input\": [HumanMessage(content=\"what is the capital of india\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77580365",
   "metadata": {},
   "source": [
    "### Examples of `LCEL`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d8f7d6",
   "metadata": {},
   "source": [
    "#### 1. How to run custom functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e841ea",
   "metadata": {},
   "source": [
    "- This is important when you want to pass arbitrary function as [Runnables](./runnable.md).\n",
    "- When you want to build a functionality, not provided by the LangChain. So custom functions are used as [RunnableLambda](./runnable.md).\n",
    "- **üíÄ Note:** All these functions only accept **\"SINGLE\"** arguments. To pass multiple you have to build wrapper that single dict as input and unpack them into multiple."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1380e828",
   "metadata": {},
   "source": [
    "### 1. Using Constructor\n",
    "\n",
    "- We wrap our custom logic using the **`RunnableLambda`** constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4311073b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 9, 'b': 3}\n",
      "content='8 + 32 equals 40.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 14, 'total_tokens': 23, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0392822090', 'finish_reason': 'stop', 'logprobs': None} id='run-40da9d88-f2cc-4cd6-b6d3-2923458938a1-0' usage_metadata={'input_tokens': 14, 'output_tokens': 9, 'total_tokens': 23, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "[1, 2, 3, 4]\n",
      "{'a': 1, 'b': 2}\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "def get_len(text: str):\n",
    "    return len(text)\n",
    "\n",
    "\n",
    "def _multiple_len_function(text1, text2):\n",
    "    return len(text1) * len(text2)\n",
    "\n",
    "\n",
    "def multiply_len_function(_dict):\n",
    "    return _multiple_len_function(_dict[\"text1\"], _dict[\"text2\"])\n",
    "\n",
    "\n",
    "def print_dict(_dict):\n",
    "    print(_dict)\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"what is {a} + {b}\")\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        \"a\": itemgetter(\"foo\") | RunnableLambda(get_len),\n",
    "        \"b\": {\n",
    "            \"text1\": itemgetter(\"foo\"), \"text2\": itemgetter(\"bar\")\n",
    "        } | RunnableLambda(multiply_len_function)\n",
    "    }\n",
    "    | prompt | llm\n",
    ")\n",
    "\n",
    "chain2 = (\n",
    "    {\n",
    "        \"a\": {\n",
    "            \"text1\": itemgetter(\"a\"),  # 3\n",
    "            \"text2\": itemgetter(\"b\")  # 3 ==> 3 * 3 = 9\n",
    "        } | RunnableLambda(multiply_len_function),\n",
    "        \"b\": itemgetter(\"b\") | RunnableLambda(lambda x: len(x))\n",
    "    } | RunnableLambda(print_dict)\n",
    ")\n",
    "\n",
    "\n",
    "chain2.invoke({\"a\": \"bob\", \"b\": \"pop\"})\n",
    "\n",
    "print(chain.invoke({\"foo\": \"abdurrab\", \"bar\": \"khan\"}))\n",
    "\n",
    "\n",
    "def hello(a: int):\n",
    "    print(\"A \", a)\n",
    "\n",
    "\n",
    "chain5 = RunnableLambda(hello)\n",
    "\n",
    "# chain5.invoke(2, 5) ‚ö†Ô∏è We can't pass more than one arguments, but dict, list can.\n",
    "\n",
    "\n",
    "def get_dict(a):\n",
    "    print(a)\n",
    "\n",
    "\n",
    "chain6 = RunnableLambda(get_dict)\n",
    "\n",
    "chain6.invoke([1, 2, 3, 4])\n",
    "chain6.invoke({\"a\": 1, \"b\": 2})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750c688",
   "metadata": {},
   "source": [
    "### The convenience `@chain` decorator\n",
    "\n",
    "- You convert arbitrary function into the chain by using **`@chain`** decorators.\n",
    "- This functionality is equivalent to wrapping the function in **`RunnableLambda`** constructor as shown above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
